{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1994,
     "status": "ok",
     "timestamp": 1679115294967,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "RvaNWdrKgXjD",
    "outputId": "42afdbf9-c1a0-474c-d094-a237c1063f70"
   },
   "outputs": [],
   "source": [
    "#We extract the pubmed document in BioCJSON format\n",
    "import urllib3\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from itertools import combinations\n",
    "\n",
    "pmcid = 'PMC2837563'\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "r = http.request('GET', f'https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocjson?pmcids={pmcid}')\n",
    "data = json.loads(r.data.decode('utf-8'))\n",
    "#data = json.dumps(data, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1679115294968,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "-3bUHWus1nUq",
    "outputId": "6459e92e-d045-4841-eb58-35ac0f68d993"
   },
   "outputs": [],
   "source": [
    "output = json.dumps(data, indent=4)\n",
    "#print(output)\n",
    "\n",
    "'''\n",
    "Every PMC id has passages.\n",
    "Every passage has many {infons, offset, text, sentences, annotations, relations}.\n",
    "Here, text is the actual text we have to annotate. \n",
    "Every annotations has {id, infons, text, locations}. \n",
    "Here infons has {identifier, type} (optional: ncbi-homologene if type is gene).\n",
    "Also locations has {offset, length}.\n",
    "\n",
    "Passages:\n",
    "    a)infons - data realted article id, author, etc..\n",
    "    b)offset - location index\n",
    "    c)text - whole medical data (sentence) in which medical terms (gene name or disease name) are to be annotated.\n",
    "    d)sentences - not required here\n",
    "    e)annotations:\n",
    "        1)id - key index\n",
    "        2)infons:\n",
    "              a)identifier\n",
    "              b)type - \"Gene\" or \"Disease\" etc.,\n",
    "        3)text - gene name or disease name etc., (Eg: \"K-Ras\")\n",
    "        4)locations:\n",
    "            a)offset - location index\n",
    "            b)length - length of text. (Eg: len(\"tumours\") = 7) \n",
    "    f)relations\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EkWKX8wA1b5"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(columns=['index','sentence'])\n",
    "df2 = pd.DataFrame(columns=['index','sentence','Gene','Disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSMLCcRaAlL_"
   },
   "source": [
    "**Annotations for Gene and Mutation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbIqNKyCgkuF"
   },
   "outputs": [],
   "source": [
    "sentence_index = 0\n",
    "sentence_entities = {}\n",
    "for i in data['passages']:\n",
    "  if i['infons']['section_type'] != 'TABLE':\n",
    "    #filter the table segment\n",
    "    text = i['text']\n",
    "    # print(\"TEXT: \", text)\n",
    "    offset = i['offset']\n",
    "    # print(\"OFFSET: \", offset)    \n",
    "    annotations = i['annotations']\n",
    "    \n",
    "    annotations = sorted(annotations, key = lambda x: x['locations'][0]['offset'])\n",
    "    # print(\"ANNOTATIONS: \", annotations)\n",
    "    #Filter to only include gene-disease annotations\n",
    "    annotations = [annotation for annotation in annotations if ((annotation['infons']['type']=='Gene') or (annotation['infons']['type']=='Disease'))]\n",
    "    #List all possible combinations of annotations\n",
    "    annots_combinations = list(combinations(annotations, 2))\n",
    "    # print(\"ANNOT COMBIMNATIONS: \", annots_combinations)\n",
    "    #Filter combinations to only include gene-disease combinations\n",
    "    annots_combinations = [annots for annots in annots_combinations if annots[0]['infons']['type'] != annots[1]['infons']['type']]\n",
    "    # print(\"ANNOT COMBINATIONS - GENE-DISEASE: \", annots_combinations)\n",
    "\n",
    "    #processing sentences\n",
    "    sentences = text.split('. ')\n",
    "    # print(sentences)\n",
    "    sentence_offset = {}\n",
    "    sentence_len = {}\n",
    "    prev_sent_offset = offset\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "      sentence_offset[sentence] = prev_sent_offset\n",
    "      current_sentence_offset = prev_sent_offset\n",
    "      sentence_len[sentence] = len(sentence)\n",
    "      current_sentence_len = len(sentence)\n",
    "      prev_sent_offset += len(sentence) + 2\n",
    "      \n",
    "\n",
    "      #Point to note: Duplicate sentences for as many combinations as present. \n",
    "      for annots in annots_combinations:\n",
    "        difference = 0\n",
    "        current_sentence = sentence\n",
    "        #sort the tuple\n",
    "        annots = sorted(annots, key = lambda x: x['locations'][0]['offset'])\n",
    "\n",
    "        entity_1 = annots[0]\n",
    "        entity_2 = annots[1]\n",
    "        \n",
    "        entity_1_offset = entity_1['locations'][0]['offset']\n",
    "        entity_2_offset = entity_2['locations'][0]['offset']\n",
    "\n",
    "        entity_1_dist = entity_1_offset - current_sentence_offset\n",
    "        entity_2_dist = entity_2_offset - current_sentence_offset\n",
    "\n",
    "        if (0 <= entity_1_dist <= ((current_sentence_len - len(entity_1['text'])) + 1)) and (0 <= entity_2_dist <= ((current_sentence_len - len(entity_2['text'])) + 1)):\n",
    "          #the pair of annotations fall within the sentence\n",
    "          sentence_entities[sentence_index] = (entity_1['text'], entity_2['text'])\n",
    "\n",
    "          entity_1_type = entity_1['infons']['type']\n",
    "          entity_1_length = entity_1['locations'][0]['length']\n",
    "          temp = '@'+ entity_1_type +'$'\n",
    "          entity_1_final_off = entity_1_dist \n",
    "          current_sentence = current_sentence[:entity_1_final_off] + \"@\" + entity_1_type + \"$\" + current_sentence[(entity_1_final_off + entity_1_length):]\n",
    "          difference += (entity_1_length - len(temp))\n",
    "          entity_2_type = entity_2['infons']['type']\n",
    "          entity_2_length = entity_2['locations'][0]['length']\n",
    "          temp = '@'+ entity_2_type +'$'\n",
    "          entity_2_final_off = entity_2_dist - (difference)\n",
    "          current_sentence = current_sentence[:entity_2_final_off] + \"@\" + entity_2_type + \"$\" + current_sentence[(entity_2_final_off + entity_2_length):]\n",
    "          difference += (entity_2_length - len(temp))\n",
    "          # tsv_writer_1.writerow([sentence_index, current_sentence])\n",
    "          row1 = [sentence_index,current_sentence]\n",
    "          df1.loc[len(df1)] = row1\n",
    "          if (entity_1['infons']['type'] == 'Gene'):\n",
    "            # print('Writing...', [sentence_index, sentence, entity_1['text'], entity_2['text']])\n",
    "            # tsv_writer_2.writerow([sentence_index, sentence, entity_1['text'], entity_2['text']])\n",
    "            row2 = [sentence_index, sentence, entity_1['text'], entity_2['text']]\n",
    "            df2.loc[len(df2)] = row2\n",
    "          else:\n",
    "            # print('Writing')\n",
    "            # tsv_writer_2.writerow([sentence_index, sentence, entity_2['text'], entity_1['text']])\n",
    "            row2 = [sentence_index, sentence, entity_2['text'], entity_1['text']]\n",
    "            df2.loc[len(df2)] = row2\n",
    "          sentence_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rax5aqQmrZp-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../biobert-pytorch/relation-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe0kay_JgprA"
   },
   "outputs": [],
   "source": [
    "df2.to_csv('pub_original_sentences_GD.tsv', sep=\"\\t\", index=False)\n",
    "df1.to_csv('inputGD/test.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1679115418092,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "WqZjd-hTEHQ2",
    "outputId": "cb44b4b5-e156-4332-b823-d0cd67429652"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('pub_original_sentences_GD.tsv', sep=\"\\t\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZSYZQAAi5db"
   },
   "source": [
    "**BioBert Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ETmbMG8sDB2U"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49572,
     "status": "ok",
     "timestamp": 1679113913000,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "zIu8GyzMjUoE",
    "outputId": "fd85fe47-e028-48ca-c80d-d9d344329c9e"
   },
   "outputs": [],
   "source": [
    "# Download all datasets including NER/RE/QA\n",
    "!bash ./download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7241,
     "status": "ok",
     "timestamp": 1679113920218,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "lfbbpV5lkJry",
    "outputId": "3c42dba5-3879-4891-c84f-ffcca7fbe599"
   },
   "outputs": [],
   "source": [
    "os.chdir('../relation-extraction')\n",
    "\n",
    "#To preprocess the datasets downloaded\n",
    "!bash ./preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9321,
     "status": "ok",
     "timestamp": 1679113929525,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "jPCXvCJLkLsy",
    "outputId": "5b07e932-5654-4efb-cd39-55e358b64933"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1679120234782,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "uv7yHEmWkPk8",
    "outputId": "83a27745-c3c5-486f-f279-91040342140c"
   },
   "outputs": [],
   "source": [
    "%env SAVE_DIR=./outputGD\n",
    "%env DATA=\"GAD\"\n",
    "%env SPLIT=\"1\"\n",
    "%env DATA_DIR=./inputGD\n",
    "%env ENTITY=${DATA}-${SPLIT}\n",
    "\n",
    "%env MAX_LENGTH=128\n",
    "%env BATCH_SIZE=32\n",
    "%env NUM_EPOCHS=3\n",
    "%env SAVE_STEPS=1000\n",
    "%env SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6962,
     "status": "ok",
     "timestamp": 1679120242449,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "Nap7OsDNomOM",
    "outputId": "e92aad7a-3cdc-4402-ce07-ed5660a090a3"
   },
   "outputs": [],
   "source": [
    "!python run_re.py --task_name SST-2 --config_name bert-base-cased --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n",
    "        --do_predict --data_dir ${DATA_DIR} \\\n",
    "        --output_dir ${SAVE_DIR} \\\n",
    "        --overwrite_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1679064615235,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "QgHoYa6Noo9w",
    "outputId": "6af921bc-1802-4a23-bb95-187243510d51"
   },
   "outputs": [],
   "source": [
    "original_sentences = pd.read_csv('pub_original_sentences.tsv', sep=\"\\t\")\n",
    "predictions = pd.read_csv('output/test_results.txt', sep='\\t')\n",
    "\n",
    "#df = pd.read_csv('pub_original_sentences.tsv', sep=\"\\t\")\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX9DHYpyBbeC"
   },
   "outputs": [],
   "source": [
    "#Merge the pub original sentences and test results (prediction of biobert model)\n",
    "\n",
    "final_re_output = pd.merge(original_sentences, predictions, on ='index', how='left')\n",
    "final_re_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('outputGD')\n",
    "final_re_output.to_csv('final_GD_output.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./scripts/re_eval.py --output_path=output/test_results.txt --answer_path=output/test_original.tsv"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
