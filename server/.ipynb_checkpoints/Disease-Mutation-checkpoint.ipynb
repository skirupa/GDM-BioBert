{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3605,
     "status": "ok",
     "timestamp": 1679075791770,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "RvaNWdrKgXjD",
    "outputId": "7fe66ab5-ecad-49ad-f8f1-6398703962bc"
   },
   "outputs": [],
   "source": [
    "#We extract the pubmed document in BioCJSON format\n",
    "import urllib3\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "pmcid = 'PMC2837563'\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "r = http.request('GET', f'https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocjson?pmcids={pmcid}')\n",
    "data = json.loads(r.data.decode('utf-8'))\n",
    "#data = json.dumps(data, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1679075793852,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "-3bUHWus1nUq",
    "outputId": "3f1c4784-c8ea-473b-d8bb-aefd779ea62b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEvery PMC id has passages.\\nEvery passage has many {infons, offset, text, sentences, annotations, relations}.\\nHere, text is the actual text we have to annotate. \\nEvery annotations has {id, infons, text, locations}. \\nHere infons has {identifier, type} (optional: ncbi-homologene if type is gene).\\nAlso locations has {offset, length}.\\n\\nPassages:\\n    a)infons - data realted article id, author, etc..\\n    b)offset - location index\\n    c)text - whole medical data (sentence) in which medical terms (gene name or disease name) are to be annotated.\\n    d)sentences - not required here\\n    e)annotations:\\n        1)id - key index\\n        2)infons:\\n              a)identifier\\n              b)type - \"Gene\" or \"Disease\" etc.,\\n        3)text - gene name or disease name etc., (Eg: \"K-Ras\")\\n        4)locations:\\n            a)offset - location index\\n            b)length - length of text. (Eg: len(\"tumours\") = 7) \\n    f)relations\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = json.dumps(data, indent=4)\n",
    "#print(output)\n",
    "\n",
    "'''\n",
    "Every PMC id has passages.\n",
    "Every passage has many {infons, offset, text, sentences, annotations, relations}.\n",
    "Here, text is the actual text we have to annotate. \n",
    "Every annotations has {id, infons, text, locations}. \n",
    "Here infons has {identifier, type} (optional: ncbi-homologene if type is gene).\n",
    "Also locations has {offset, length}.\n",
    "\n",
    "Passages:\n",
    "    a)infons - data realted article id, author, etc..\n",
    "    b)offset - location index\n",
    "    c)text - whole medical data (sentence) in which medical terms (gene name or disease name) are to be annotated.\n",
    "    d)sentences - not required here\n",
    "    e)annotations:\n",
    "        1)id - key index\n",
    "        2)infons:\n",
    "              a)identifier\n",
    "              b)type - \"Gene\" or \"Disease\" etc.,\n",
    "        3)text - gene name or disease name etc., (Eg: \"K-Ras\")\n",
    "        4)locations:\n",
    "            a)offset - location index\n",
    "            b)length - length of text. (Eg: len(\"tumours\") = 7) \n",
    "    f)relations\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3EkWKX8wA1b5"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(columns=['index','sentence'])\n",
    "df2 = pd.DataFrame(columns=['index','sentence','entity_1','entity_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSMLCcRaAlL_"
   },
   "source": [
    "**Annotations for Gene and Mutation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kbIqNKyCgkuF"
   },
   "outputs": [],
   "source": [
    "sentence_index = 0\n",
    "sentence_entities = {}\n",
    "for i in data['passages']:\n",
    "  if i['infons']['section_type'] != 'TABLE':\n",
    "    #filter the table segment\n",
    "    text = i['text']\n",
    "    # print(\"TEXT: \", text)\n",
    "    offset = i['offset']\n",
    "    # print(\"OFFSET: \", offset)    \n",
    "    annotations = i['annotations']\n",
    "    \n",
    "    annotations = sorted(annotations, key = lambda x: x['locations'][0]['offset'])\n",
    "    # print(\"ANNOTATIONS: \", annotations)\n",
    "    #Filter to only include gene-disease annotations\n",
    "    annotations = [annotation for annotation in annotations if ((annotation['infons']['type']=='Disease') or (annotation['infons']['type']=='Mutation'))]\n",
    "    #List all possible combinations of annotations\n",
    "    annots_combinations = list(combinations(annotations, 2))\n",
    "    # print(\"ANNOT COMBIMNATIONS: \", annots_combinations)\n",
    "    #Filter combinations to only include gene-disease combinations\n",
    "    annots_combinations = [annots for annots in annots_combinations if annots[0]['infons']['type'] != annots[1]['infons']['type']]\n",
    "    # print(\"ANNOT COMBINATIONS - GENE-DISEASE: \", annots_combinations)\n",
    "\n",
    "    #processing sentences\n",
    "    sentences = text.split('. ')\n",
    "    # print(sentences)\n",
    "    sentence_offset = {}\n",
    "    sentence_len = {}\n",
    "    prev_sent_offset = offset\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "      sentence_offset[sentence] = prev_sent_offset\n",
    "      current_sentence_offset = prev_sent_offset\n",
    "      sentence_len[sentence] = len(sentence)\n",
    "      current_sentence_len = len(sentence)\n",
    "      prev_sent_offset += len(sentence) + 2\n",
    "      \n",
    "\n",
    "      #Point to note: Duplicate sentences for as many combinations as present. \n",
    "      for annots in annots_combinations:\n",
    "        difference = 0\n",
    "        current_sentence = sentence\n",
    "        #sort the tuple\n",
    "        annots = sorted(annots, key = lambda x: x['locations'][0]['offset'])\n",
    "\n",
    "        entity_1 = annots[0]\n",
    "        entity_2 = annots[1]\n",
    "        \n",
    "        entity_1_offset = entity_1['locations'][0]['offset']\n",
    "        entity_2_offset = entity_2['locations'][0]['offset']\n",
    "\n",
    "        entity_1_dist = entity_1_offset - current_sentence_offset\n",
    "        entity_2_dist = entity_2_offset - current_sentence_offset\n",
    "\n",
    "        if (0 <= entity_1_dist <= ((current_sentence_len - len(entity_1['text'])) + 1)) and (0 <= entity_2_dist <= ((current_sentence_len - len(entity_2['text'])) + 1)):\n",
    "          #the pair of annotations fall within the sentence\n",
    "          sentence_entities[sentence_index] = (entity_1['text'], entity_2['text'])\n",
    "\n",
    "          entity_1_type = entity_1['infons']['type']\n",
    "          entity_1_length = entity_1['locations'][0]['length']\n",
    "          temp = '@'+ entity_1_type +'$'\n",
    "          entity_1_final_off = entity_1_dist \n",
    "          current_sentence = current_sentence[:entity_1_final_off] + \"@\" + entity_1_type + \"$\" + current_sentence[(entity_1_final_off + entity_1_length):]\n",
    "          difference += (entity_1_length - len(temp))\n",
    "          entity_2_type = entity_2['infons']['type']\n",
    "          entity_2_length = entity_2['locations'][0]['length']\n",
    "          temp = '@'+ entity_2_type +'$'\n",
    "          entity_2_final_off = entity_2_dist - (difference)\n",
    "          current_sentence = current_sentence[:entity_2_final_off] + \"@\" + entity_2_type + \"$\" + current_sentence[(entity_2_final_off + entity_2_length):]\n",
    "          difference += (entity_2_length - len(temp))\n",
    "          # tsv_writer_1.writerow([sentence_index, current_sentence])\n",
    "          row1 = [sentence_index,current_sentence]\n",
    "          df1.loc[len(df1)] = row1\n",
    "          if (entity_1['infons']['type'] == 'Disease'):\n",
    "            # print('Writing...', [sentence_index, sentence, entity_1['text'], entity_2['text']])\n",
    "            # tsv_writer_2.writerow([sentence_index, sentence, entity_1['text'], entity_2['text']])\n",
    "            row2 = [sentence_index, sentence, entity_1['text'], entity_2['text']]\n",
    "            df2.loc[len(df2)] = row2\n",
    "          else:\n",
    "            # print('Writing')\n",
    "            # tsv_writer_2.writerow([sentence_index, sentence, entity_2['text'], entity_1['text']])\n",
    "            row2 = [sentence_index, sentence, entity_2['text'], entity_1['text']]\n",
    "            df2.loc[len(df2)] = row2\n",
    "          sentence_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rax5aqQmrZp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/skirupa/Desktop/sem8/RSL-Lab/RSLLab-20230318T070434Z-001/RSLLab/biobert-pytorch/relation-extraction\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../biobert-pytorch/relation-extraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4a21b851cb77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../biobert-pytorch/relation-extraction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../biobert-pytorch/relation-extraction'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('../biobert-pytorch/relation-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe0kay_JgprA"
   },
   "outputs": [],
   "source": [
    "df2.to_csv('pub_original_sentences_DM.tsv', sep=\"\\t\", index=False)\n",
    "df1.to_csv('inputDM/test.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZSYZQAAi5db"
   },
   "source": [
    "**BioBert Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all datasets including NER/RE/QA\n",
    "#!bash ./download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('relation-extraction')\n",
    "print(os.getcwd())\n",
    "#To preprocess the datasets downloaded\n",
    "!bash ./preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8981,
     "status": "ok",
     "timestamp": 1679070353775,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "jPCXvCJLkLsy",
    "outputId": "b2ffa5ef-ecb7-4bd9-dbe3-640d4f36ddcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1679070381883,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "uv7yHEmWkPk8",
    "outputId": "678d9197-a2d7-41d0-fa94-9dc9a2532ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SAVE_DIR=./outputDM\n",
      "env: DATA=\"GAD\"\n",
      "env: SPLIT=\"1\"\n",
      "env: DATA_DIR=./inputDM\n",
      "env: ENTITY=${DATA}-${SPLIT}\n",
      "env: MAX_LENGTH=128\n",
      "env: BATCH_SIZE=32\n",
      "env: NUM_EPOCHS=3\n",
      "env: SAVE_STEPS=1000\n",
      "env: SEED=1\n"
     ]
    }
   ],
   "source": [
    "%env SAVE_DIR=./outputDM\n",
    "%env DATA=\"GAD\"\n",
    "%env SPLIT=\"1\"\n",
    "%env DATA_DIR=./inputDM\n",
    "%env ENTITY=${DATA}-${SPLIT}\n",
    "\n",
    "%env MAX_LENGTH=128\n",
    "%env BATCH_SIZE=32\n",
    "%env NUM_EPOCHS=3\n",
    "%env SAVE_STEPS=1000\n",
    "%env SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7095,
     "status": "ok",
     "timestamp": 1679070394106,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "Nap7OsDNomOM",
    "outputId": "4ba15580-e24e-4619-f13d-fb35ad41f6ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/18/2023 15:42:28 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "Traceback (most recent call last):\n",
      "  File \"run_re.py\", line 259, in <module>\n",
      "    main()\n",
      "  File \"run_re.py\", line 134, in main\n",
      "    if training_args.do_predict\n",
      "  File \"/home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages/transformers/data/datasets/glue.py\", line 121, in __init__\n",
      "    examples = self.processor.get_test_examples(args.data_dir)\n",
      "  File \"/home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages/transformers/data/processors/glue.py\", line 324, in get_test_examples\n",
      "    return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
      "  File \"/home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages/transformers/data/processors/glue.py\", line 338, in _create_examples\n",
      "    text_a = line[text_index]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!python run_re.py --task_name SST-2 --config_name bert-base-cased --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n",
    "        --do_predict --data_dir ${DATA_DIR} \\\n",
    "        --output_dir ${SAVE_DIR} \\\n",
    "        --overwrite_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1679064615235,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "QgHoYa6Noo9w",
    "outputId": "6af921bc-1802-4a23-bb95-187243510d51"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b865afc6ab6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moriginal_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pub_original_sentences.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/test_results.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/test_results.txt'"
     ]
    }
   ],
   "source": [
    "original_sentences = pd.read_csv('pub_original_sentences_DM.tsv', sep=\"\\t\")\n",
    "predictions = pd.read_csv('outputDM/test_results.txt', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX9DHYpyBbeC"
   },
   "outputs": [],
   "source": [
    "#Merge the pub original sentences and test results (prediction of biobert model)\n",
    "\n",
    "final_re_output = pd.merge(original_sentences, predictions, on ='index', how='left')\n",
    "final_re_output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
