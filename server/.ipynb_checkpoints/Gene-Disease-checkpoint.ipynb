{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1994,
     "status": "ok",
     "timestamp": 1679115294967,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "RvaNWdrKgXjD",
    "outputId": "42afdbf9-c1a0-474c-d094-a237c1063f70"
   },
   "outputs": [],
   "source": [
    "#We extract the pubmed document in BioCJSON format\n",
    "import urllib3\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from itertools import combinations\n",
    "\n",
    "pmcid = 'PMC2837563'\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "r = http.request('GET', f'https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocjson?pmcids={pmcid}')\n",
    "data = json.loads(r.data.decode('utf-8'))\n",
    "#data = json.dumps(data, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1679115294968,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "-3bUHWus1nUq",
    "outputId": "6459e92e-d045-4841-eb58-35ac0f68d993"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEvery PMC id has passages.\\nEvery passage has many {infons, offset, text, sentences, annotations, relations}.\\nHere, text is the actual text we have to annotate. \\nEvery annotations has {id, infons, text, locations}. \\nHere infons has {identifier, type} (optional: ncbi-homologene if type is gene).\\nAlso locations has {offset, length}.\\n\\nPassages:\\n    a)infons - data realted article id, author, etc..\\n    b)offset - location index\\n    c)text - whole medical data (sentence) in which medical terms (gene name or disease name) are to be annotated.\\n    d)sentences - not required here\\n    e)annotations:\\n        1)id - key index\\n        2)infons:\\n              a)identifier\\n              b)type - \"Gene\" or \"Disease\" etc.,\\n        3)text - gene name or disease name etc., (Eg: \"K-Ras\")\\n        4)locations:\\n            a)offset - location index\\n            b)length - length of text. (Eg: len(\"tumours\") = 7) \\n    f)relations\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = json.dumps(data, indent=4)\n",
    "#print(output)\n",
    "\n",
    "'''\n",
    "Every PMC id has passages.\n",
    "Every passage has many {infons, offset, text, sentences, annotations, relations}.\n",
    "Here, text is the actual text we have to annotate. \n",
    "Every annotations has {id, infons, text, locations}. \n",
    "Here infons has {identifier, type} (optional: ncbi-homologene if type is gene).\n",
    "Also locations has {offset, length}.\n",
    "\n",
    "Passages:\n",
    "    a)infons - data realted article id, author, etc..\n",
    "    b)offset - location index\n",
    "    c)text - whole medical data (sentence) in which medical terms (gene name or disease name) are to be annotated.\n",
    "    d)sentences - not required here\n",
    "    e)annotations:\n",
    "        1)id - key index\n",
    "        2)infons:\n",
    "              a)identifier\n",
    "              b)type - \"Gene\" or \"Disease\" etc.,\n",
    "        3)text - gene name or disease name etc., (Eg: \"K-Ras\")\n",
    "        4)locations:\n",
    "            a)offset - location index\n",
    "            b)length - length of text. (Eg: len(\"tumours\") = 7) \n",
    "    f)relations\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3EkWKX8wA1b5"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(columns=['index','sentence'])\n",
    "df2 = pd.DataFrame(columns=['index','sentence','entity_1','entity_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSMLCcRaAlL_"
   },
   "source": [
    "**Annotations for Gene and Mutation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kbIqNKyCgkuF"
   },
   "outputs": [],
   "source": [
    "sentence_index = 0\n",
    "sentence_entities = {}\n",
    "for i in data['passages']:\n",
    "  if i['infons']['section_type'] != 'TABLE':\n",
    "    #filter the table segment\n",
    "    text = i['text']\n",
    "    # print(\"TEXT: \", text)\n",
    "    offset = i['offset']\n",
    "    # print(\"OFFSET: \", offset)    \n",
    "    annotations = i['annotations']\n",
    "    \n",
    "    annotations = sorted(annotations, key = lambda x: x['locations'][0]['offset'])\n",
    "    # print(\"ANNOTATIONS: \", annotations)\n",
    "    #Filter to only include gene-disease annotations\n",
    "    annotations = [annotation for annotation in annotations if ((annotation['infons']['type']=='Gene') or (annotation['infons']['type']=='Mutation'))]\n",
    "    #List all possible combinations of annotations\n",
    "    annots_combinations = list(combinations(annotations, 2))\n",
    "    # print(\"ANNOT COMBIMNATIONS: \", annots_combinations)\n",
    "    #Filter combinations to only include gene-disease combinations\n",
    "    annots_combinations = [annots for annots in annots_combinations if annots[0]['infons']['type'] != annots[1]['infons']['type']]\n",
    "    # print(\"ANNOT COMBINATIONS - GENE-DISEASE: \", annots_combinations)\n",
    "\n",
    "    #processing sentences\n",
    "    sentences = text.split('. ')\n",
    "    # print(sentences)\n",
    "    sentence_offset = {}\n",
    "    sentence_len = {}\n",
    "    prev_sent_offset = offset\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "      sentence_offset[sentence] = prev_sent_offset\n",
    "      current_sentence_offset = prev_sent_offset\n",
    "      sentence_len[sentence] = len(sentence)\n",
    "      current_sentence_len = len(sentence)\n",
    "      prev_sent_offset += len(sentence) + 2\n",
    "      \n",
    "\n",
    "      #Point to note: Duplicate sentences for as many combinations as present. \n",
    "      for annots in annots_combinations:\n",
    "        difference = 0\n",
    "        current_sentence = sentence\n",
    "        #sort the tuple\n",
    "        annots = sorted(annots, key = lambda x: x['locations'][0]['offset'])\n",
    "\n",
    "        entity_1 = annots[0]\n",
    "        entity_2 = annots[1]\n",
    "        \n",
    "        entity_1_offset = entity_1['locations'][0]['offset']\n",
    "        entity_2_offset = entity_2['locations'][0]['offset']\n",
    "\n",
    "        entity_1_dist = entity_1_offset - current_sentence_offset\n",
    "        entity_2_dist = entity_2_offset - current_sentence_offset\n",
    "\n",
    "        if (0 <= entity_1_dist <= ((current_sentence_len - len(entity_1['text'])) + 1)) and (0 <= entity_2_dist <= ((current_sentence_len - len(entity_2['text'])) + 1)):\n",
    "          #the pair of annotations fall within the sentence\n",
    "          sentence_entities[sentence_index] = (entity_1['text'], entity_2['text'])\n",
    "\n",
    "          entity_1_type = entity_1['infons']['type']\n",
    "          entity_1_length = entity_1['locations'][0]['length']\n",
    "          temp = '@'+ entity_1_type +'$'\n",
    "          entity_1_final_off = entity_1_dist \n",
    "          current_sentence = current_sentence[:entity_1_final_off] + \"@\" + entity_1_type + \"$\" + current_sentence[(entity_1_final_off + entity_1_length):]\n",
    "          difference += (entity_1_length - len(temp))\n",
    "          entity_2_type = entity_2['infons']['type']\n",
    "          entity_2_length = entity_2['locations'][0]['length']\n",
    "          temp = '@'+ entity_2_type +'$'\n",
    "          entity_2_final_off = entity_2_dist - (difference)\n",
    "          current_sentence = current_sentence[:entity_2_final_off] + \"@\" + entity_2_type + \"$\" + current_sentence[(entity_2_final_off + entity_2_length):]\n",
    "          difference += (entity_2_length - len(temp))\n",
    "          # tsv_writer_1.writerow([sentence_index, current_sentence])\n",
    "          row1 = [sentence_index,current_sentence]\n",
    "          df1.loc[len(df1)] = row1\n",
    "          if (entity_1['infons']['type'] == 'Gene'):\n",
    "            # print('Writing...', [sentence_index, sentence, entity_1['text'], entity_2['text']])\n",
    "            # tsv_writer_2.writerow([sentence_index, sentence, entity_1['text'], entity_2['text']])\n",
    "            row2 = [sentence_index, sentence, entity_1['text'], entity_2['text']]\n",
    "            df2.loc[len(df2)] = row2\n",
    "          else:\n",
    "            # print('Writing')\n",
    "            # tsv_writer_2.writerow([sentence_index, sentence, entity_2['text'], entity_1['text']])\n",
    "            row2 = [sentence_index, sentence, entity_2['text'], entity_1['text']]\n",
    "            df2.loc[len(df2)] = row2\n",
    "          sentence_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rax5aqQmrZp-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../biobert-pytorch/relation-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Oe0kay_JgprA"
   },
   "outputs": [],
   "source": [
    "df2.to_csv('pub_original_sentences.tsv', sep=\"\\t\", index=False)\n",
    "df1.to_csv('input/test.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1679115418092,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "WqZjd-hTEHQ2",
    "outputId": "cb44b4b5-e156-4332-b823-d0cd67429652"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'sentence', 'entity_1', 'entity_2'], dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pub_original_sentences.tsv', sep=\"\\t\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZSYZQAAi5db"
   },
   "source": [
    "**BioBert Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ETmbMG8sDB2U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/skirupa/Desktop/sem8/RSL-Lab/RSLLab-20230318T070434Z-001/RSLLab/biobert-pytorch/relation-extraction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49572,
     "status": "ok",
     "timestamp": 1679113913000,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "zIu8GyzMjUoE",
    "outputId": "fd85fe47-e028-48ca-c80d-d9d344329c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIOBERT_DATA not set; downloading to default path ('data').\n",
      "--2023-03-18 13:34:45--  http://nlp.dmis.korea.edu/projects/biobert-2020-checkpoints/datasets.tar.gz\n",
      "Resolving nlp.dmis.korea.edu (nlp.dmis.korea.edu)... 163.152.163.168\n",
      "Connecting to nlp.dmis.korea.edu (nlp.dmis.korea.edu)|163.152.163.168|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29610233 (28M) [application/x-gzip]\n",
      "Saving to: ‘./data.tar.gz’\n",
      "\n",
      "./data.tar.gz       100%[===================>]  28.24M   615KB/s    in 70s     \n",
      "\n",
      "2023-03-18 13:35:56 (413 KB/s) - ‘./data.tar.gz’ saved [29610233/29610233]\n",
      "\n",
      "datasets/\n",
      "datasets/RE/\n",
      "datasets/RE/GAD/\n",
      "datasets/RE/GAD/6/\n",
      "datasets/RE/GAD/6/test.tsv\n",
      "datasets/RE/GAD/6/dev.tsv\n",
      "datasets/RE/GAD/6/train.tsv\n",
      "datasets/RE/GAD/7/\n",
      "datasets/RE/GAD/7/test.tsv\n",
      "datasets/RE/GAD/7/dev.tsv\n",
      "datasets/RE/GAD/7/train.tsv\n",
      "datasets/RE/GAD/5/\n",
      "datasets/RE/GAD/5/test.tsv\n",
      "datasets/RE/GAD/5/dev.tsv\n",
      "datasets/RE/GAD/5/train.tsv\n",
      "datasets/RE/GAD/8/\n",
      "datasets/RE/GAD/8/test.tsv\n",
      "datasets/RE/GAD/8/dev.tsv\n",
      "datasets/RE/GAD/8/train.tsv\n",
      "datasets/RE/GAD/4/\n",
      "datasets/RE/GAD/4/test.tsv\n",
      "datasets/RE/GAD/4/dev.tsv\n",
      "datasets/RE/GAD/4/train.tsv\n",
      "datasets/RE/GAD/1/\n",
      "datasets/RE/GAD/1/test.tsv\n",
      "datasets/RE/GAD/1/dev.tsv\n",
      "datasets/RE/GAD/1/train.tsv\n",
      "datasets/RE/GAD/2/\n",
      "datasets/RE/GAD/2/test.tsv\n",
      "datasets/RE/GAD/2/dev.tsv\n",
      "datasets/RE/GAD/2/train.tsv\n",
      "datasets/RE/GAD/3/\n",
      "datasets/RE/GAD/3/test.tsv\n",
      "datasets/RE/GAD/3/dev.tsv\n",
      "datasets/RE/GAD/3/train.tsv\n",
      "datasets/RE/GAD/9/\n",
      "datasets/RE/GAD/9/test.tsv\n",
      "datasets/RE/GAD/9/dev.tsv\n",
      "datasets/RE/GAD/9/train.tsv\n",
      "datasets/RE/GAD/10/\n",
      "datasets/RE/GAD/10/test.tsv\n",
      "datasets/RE/GAD/10/dev.tsv\n",
      "datasets/RE/GAD/10/train.tsv\n",
      "datasets/RE/euadr/\n",
      "datasets/RE/euadr/6/\n",
      "datasets/RE/euadr/6/test.tsv\n",
      "datasets/RE/euadr/6/dev.tsv\n",
      "datasets/RE/euadr/6/train.tsv\n",
      "datasets/RE/euadr/7/\n",
      "datasets/RE/euadr/7/test.tsv\n",
      "datasets/RE/euadr/7/dev.tsv\n",
      "datasets/RE/euadr/7/train.tsv\n",
      "datasets/RE/euadr/5/\n",
      "datasets/RE/euadr/5/test.tsv\n",
      "datasets/RE/euadr/5/dev.tsv\n",
      "datasets/RE/euadr/5/train.tsv\n",
      "datasets/RE/euadr/8/\n",
      "datasets/RE/euadr/8/test.tsv\n",
      "datasets/RE/euadr/8/dev.tsv\n",
      "datasets/RE/euadr/8/train.tsv\n",
      "datasets/RE/euadr/4/\n",
      "datasets/RE/euadr/4/test.tsv\n",
      "datasets/RE/euadr/4/dev.tsv\n",
      "datasets/RE/euadr/4/train.tsv\n",
      "datasets/RE/euadr/1/\n",
      "datasets/RE/euadr/1/test.tsv\n",
      "datasets/RE/euadr/1/dev.tsv\n",
      "datasets/RE/euadr/1/train.tsv\n",
      "datasets/RE/euadr/2/\n",
      "datasets/RE/euadr/2/test.tsv\n",
      "datasets/RE/euadr/2/dev.tsv\n",
      "datasets/RE/euadr/2/train.tsv\n",
      "datasets/RE/euadr/3/\n",
      "datasets/RE/euadr/3/test.tsv\n",
      "datasets/RE/euadr/3/dev.tsv\n",
      "datasets/RE/euadr/3/train.tsv\n",
      "datasets/RE/euadr/9/\n",
      "datasets/RE/euadr/9/test.tsv\n",
      "datasets/RE/euadr/9/dev.tsv\n",
      "datasets/RE/euadr/9/train.tsv\n",
      "datasets/RE/euadr/10/\n",
      "datasets/RE/euadr/10/test.tsv\n",
      "datasets/RE/euadr/10/dev.tsv\n",
      "datasets/RE/euadr/10/train.tsv\n",
      "datasets/QA/\n",
      "datasets/QA/BioASQ/\n",
      "datasets/QA/BioASQ/6B1_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-7b.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-5b-4.json\n",
      "datasets/QA/BioASQ/5B3_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-4b-2.json\n",
      "datasets/QA/BioASQ/4B1_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-train-yesno-7b.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-5b-5.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-5b-2.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-6b-2.json\n",
      "datasets/QA/BioASQ/4B4_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-6b-3.json\n",
      "datasets/QA/BioASQ/6B3_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-5b-3.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-6b-5.json\n",
      "datasets/QA/BioASQ/BioASQ-test-yesno-7b.json\n",
      "datasets/QA/BioASQ/BioASQ-train-factoid-4b.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-4b-1.json\n",
      "datasets/QA/BioASQ/6B5_golden.json\n",
      "datasets/QA/BioASQ/5B1_golden.json\n",
      "datasets/QA/BioASQ/4B5_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-4b-3.json\n",
      "datasets/QA/BioASQ/7B_golden.json\n",
      "datasets/QA/BioASQ/5B4_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-train-factoid-6b.json\n",
      "datasets/QA/BioASQ/BioASQ-train-factoid-7b.json\n",
      "datasets/QA/BioASQ/SHA1.txt\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-6b-1.json\n",
      "datasets/QA/BioASQ/BioASQ-train-factoid-5b.json\n",
      "datasets/QA/BioASQ/5B2_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-6b-4.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-5b-1.json\n",
      "datasets/QA/BioASQ/4B3_golden.json\n",
      "datasets/QA/BioASQ/6B2_golden.json\n",
      "datasets/QA/BioASQ/4B2_golden.json\n",
      "datasets/QA/BioASQ/6B4_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-4b-5.json\n",
      "datasets/QA/BioASQ/5B5_golden.json\n",
      "datasets/QA/BioASQ/BioASQ-test-factoid-4b-4.json\n",
      "datasets/NER/\n",
      "datasets/NER/linnaeus/\n",
      "datasets/NER/linnaeus/devel.tsv\n",
      "datasets/NER/linnaeus/test.tsv\n",
      "datasets/NER/linnaeus/train_dev.tsv\n",
      "datasets/NER/linnaeus/train.tsv\n",
      "datasets/NER/BC2GM/\n",
      "datasets/NER/BC2GM/devel.tsv\n",
      "datasets/NER/BC2GM/test.tsv\n",
      "datasets/NER/BC2GM/train_dev.tsv\n",
      "datasets/NER/BC2GM/train.tsv\n",
      "datasets/NER/BC5CDR-disease/\n",
      "datasets/NER/BC5CDR-disease/devel.tsv\n",
      "datasets/NER/BC5CDR-disease/test.tsv\n",
      "datasets/NER/BC5CDR-disease/train_dev.tsv\n",
      "datasets/NER/BC5CDR-disease/train.tsv\n",
      "datasets/NER/NCBI-disease/\n",
      "datasets/NER/NCBI-disease/devel.tsv\n",
      "datasets/NER/NCBI-disease/test.tsv\n",
      "datasets/NER/NCBI-disease/train_dev.tsv\n",
      "datasets/NER/NCBI-disease/train.tsv\n",
      "datasets/NER/s800/\n",
      "datasets/NER/s800/devel.tsv\n",
      "datasets/NER/s800/test.tsv\n",
      "datasets/NER/s800/train_dev.tsv\n",
      "datasets/NER/s800/train.tsv\n",
      "datasets/NER/BC5CDR-chem/\n",
      "datasets/NER/BC5CDR-chem/devel.tsv\n",
      "datasets/NER/BC5CDR-chem/test.tsv\n",
      "datasets/NER/BC5CDR-chem/train_dev.tsv\n",
      "datasets/NER/BC5CDR-chem/train.tsv\n",
      "datasets/NER/JNLPBA/\n",
      "datasets/NER/JNLPBA/devel.tsv\n",
      "datasets/NER/JNLPBA/test.tsv\n",
      "datasets/NER/JNLPBA/train_dev.tsv\n",
      "datasets/NER/JNLPBA/train.tsv\n",
      "datasets/NER/BC4CHEMD/\n",
      "datasets/NER/BC4CHEMD/devel.tsv\n",
      "datasets/NER/BC4CHEMD/test.tsv\n",
      "datasets/NER/BC4CHEMD/train_dev.tsv\n",
      "datasets/NER/BC4CHEMD/train.tsv\n",
      "BioBERT dataset download done!\n"
     ]
    }
   ],
   "source": [
    "# Download all datasets including NER/RE/QA\n",
    "!bash ./download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7241,
     "status": "ok",
     "timestamp": 1679113920218,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "lfbbpV5lkJry",
    "outputId": "3c42dba5-3879-4891-c84f-ffcca7fbe599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****  euadr  Preprocessing Start *****\n",
      "*****  euadr  Preprocessing Done *****\n",
      "*****  GAD  Preprocessing Start *****\n",
      "*****  GAD  Preprocessing Done *****\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../relation-extraction')\n",
    "\n",
    "#To preprocess the datasets downloaded\n",
    "!bash ./preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9321,
     "status": "ok",
     "timestamp": 1679113929525,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "jPCXvCJLkLsy",
    "outputId": "5b07e932-5654-4efb-cd39-55e358b64933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.2 MB 85 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from scikit-learn) (1.1.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 51 kB/s eta 0:00:013     |████████████████                | 12.9 MB 751 kB/s eta 0:00:18\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from scikit-learn) (1.19.5)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-0.24.2 scipy-1.5.4 threadpoolctl-3.1.0\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 7.4 kB/s eta 0:00:01     |██████████████▋                 | 4.3 MB 3.5 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "\u001b[K     |████████████████████████████████| 499 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.5 pytz-2022.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 3.0 kB/s eta 0:00:011   |▌                               | 13.9 MB 1.5 MB/s eta 0:09:56     |█▎                              | 35.8 MB 3.5 MB/s eta 0:04:03     |███                             | 80.6 MB 897 kB/s eta 0:14:53     |██████████████▎                 | 393.6 MB 1.8 MB/s eta 0:04:25     |███████████████▌                | 425.9 MB 1.9 MB/s eta 0:04:03     |████████████████▌               | 454.2 MB 475 kB/s eta 0:15:00     |██████████████████▌             | 509.5 MB 2.0 MB/s eta 0:03:11     |███████████████████▉            | 548.0 MB 1.7 MB/s eta 0:03:18     |████████████████████▏           | 557.0 MB 2.5 MB/s eta 0:02:11     |█████████████████████▊          | 597.4 MB 2.0 MB/s eta 0:02:26     |███████████████████████████▉    | 766.6 MB 1.8 MB/s eta 0:01:05     |███████████████████████████████ | 853.7 MB 1.0 MB/s eta 0:00:28\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.11.2-cp36-cp36m-manylinux1_x86_64.whl (23.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.3 MB 78 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-0.10.1-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from torch) (4.1.1)\n",
      "Collecting pillow!=8.3.0,>=5.3.0\n",
      "  Downloading Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 771 kB/s eta 0:00:01     |█████████████████████████████▌  | 2.8 MB 80 kB/s eta 0:00:03\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 3.3 kB/s eta 0:00:012    |███▊                            | 101.1 MB 721 kB/s eta 0:18:02     |█████▏                          | 140.9 MB 880 kB/s eta 0:14:02     |█████████▍                      | 260.0 MB 1.1 MB/s eta 0:09:46     |███████████▏                    | 307.4 MB 1.2 MB/s eta 0:07:56     |███████████▎                    | 312.3 MB 507 kB/s eta 0:18:44     |█████████████▋                  | 375.3 MB 474 kB/s eta 0:17:47     |████████████████▉               | 464.1 MB 75 kB/s eta 1:31:45     |█████████████████████▏          | 583.5 MB 1.5 MB/s eta 0:03:26     |██████████████████████▏         | 611.1 MB 1.4 MB/s eta 0:03:17     |██████████████████████▎         | 614.6 MB 725 kB/s eta 0:06:09     |██████████████████████▍         | 615.8 MB 1.6 MB/s eta 0:02:52��███▍    | 754.5 MB 59 kB/s eta 0:35:36     |████████████████████████████▋   | 788.4 MB 704 kB/s eta 0:02:13     |█████████████████████████████▏  | 803.2 MB 1.1 MB/s eta 0:01:14\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from torchvision) (1.19.5)\n",
      "Installing collected packages: torch, pillow, torchvision, torchaudio\n",
      "Successfully installed pillow-8.4.0 torch-1.10.1 torchaudio-0.10.1 torchvision-0.11.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1679120234782,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "uv7yHEmWkPk8",
    "outputId": "83a27745-c3c5-486f-f279-91040342140c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SAVE_DIR=./output\n",
      "env: DATA=\"GAD\"\n",
      "env: SPLIT=\"1\"\n",
      "env: DATA_DIR=./input\n",
      "env: ENTITY=${DATA}-${SPLIT}\n",
      "env: MAX_LENGTH=128\n",
      "env: BATCH_SIZE=32\n",
      "env: NUM_EPOCHS=3\n",
      "env: SAVE_STEPS=1000\n",
      "env: SEED=1\n"
     ]
    }
   ],
   "source": [
    "%env SAVE_DIR=./output\n",
    "%env DATA=\"GAD\"\n",
    "%env SPLIT=\"1\"\n",
    "%env DATA_DIR=./input\n",
    "%env ENTITY=${DATA}-${SPLIT}\n",
    "\n",
    "%env MAX_LENGTH=128\n",
    "%env BATCH_SIZE=32\n",
    "%env NUM_EPOCHS=3\n",
    "%env SAVE_STEPS=1000\n",
    "%env SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6962,
     "status": "ok",
     "timestamp": 1679120242449,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "Nap7OsDNomOM",
    "outputId": "e92aad7a-3cdc-4402-ce07-ed5660a090a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/18/2023 15:04:52 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "03/18/2023 15:04:59 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='./output', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar18_15-04-52_skirupa', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='./output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "03/18/2023 15:05:05 - INFO - root -   *** Test ***\n",
      "100%|███████████████████████████████████████████| 15/15 [00:25<00:00,  1.94s/it]/home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages/transformers/trainer.py:1118: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
      "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
      "03/18/2023 15:05:33 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "100%|███████████████████████████████████████████| 15/15 [00:25<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "!python run_re.py --task_name SST-2 --config_name bert-base-cased --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n",
    "        --do_predict --data_dir ${DATA_DIR} \\\n",
    "        --output_dir ${SAVE_DIR} \\\n",
    "        --overwrite_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1679064615235,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "QgHoYa6Noo9w",
    "outputId": "6af921bc-1802-4a23-bb95-187243510d51"
   },
   "outputs": [],
   "source": [
    "original_sentences = pd.read_csv('pub_original_sentences.tsv', sep=\"\\t\")\n",
    "predictions = pd.read_csv('output/test_results.txt', sep='\\t')\n",
    "\n",
    "#df = pd.read_csv('pub_original_sentences.tsv', sep=\"\\t\")\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QX9DHYpyBbeC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>entity_1</th>\n",
       "      <th>entity_2</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Four additional K-Ras mutations (Leu19Phe (1 o...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>Leu19Phe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Four additional K-Ras mutations (Leu19Phe (1 o...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>Lys117Asn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Four additional K-Ras mutations (Leu19Phe (1 o...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>Ala146Thr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Four additional K-Ras mutations (Leu19Phe (1 o...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>Arg164Gln</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Lys117Asn and Ala146Thr had phenotypes similar...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>Lys117Asn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>(B) The transforming potential of L19F, K117N,...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>A146T</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>116</td>\n",
       "      <td>(B) The transforming potential of L19F, K117N,...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>R164Q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>(B) The transforming potential of L19F, K117N,...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>G12V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>The K-Ras G12V construct was included as a pos...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>G12V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>The K-Ras G12V construct was included as a pos...</td>\n",
       "      <td>K-Ras</td>\n",
       "      <td>G12V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                           sentence entity_1  \\\n",
       "0        0  Four additional K-Ras mutations (Leu19Phe (1 o...    K-Ras   \n",
       "1        1  Four additional K-Ras mutations (Leu19Phe (1 o...    K-Ras   \n",
       "2        2  Four additional K-Ras mutations (Leu19Phe (1 o...    K-Ras   \n",
       "3        3  Four additional K-Ras mutations (Leu19Phe (1 o...    K-Ras   \n",
       "4        4  Lys117Asn and Ala146Thr had phenotypes similar...    K-Ras   \n",
       "..     ...                                                ...      ...   \n",
       "115    115  (B) The transforming potential of L19F, K117N,...    K-Ras   \n",
       "116    116  (B) The transforming potential of L19F, K117N,...    K-Ras   \n",
       "117    117  (B) The transforming potential of L19F, K117N,...    K-Ras   \n",
       "118    118  The K-Ras G12V construct was included as a pos...    K-Ras   \n",
       "119    119  The K-Ras G12V construct was included as a pos...    K-Ras   \n",
       "\n",
       "      entity_2  prediction  \n",
       "0     Leu19Phe           1  \n",
       "1    Lys117Asn           1  \n",
       "2    Ala146Thr           1  \n",
       "3    Arg164Gln           1  \n",
       "4    Lys117Asn           1  \n",
       "..         ...         ...  \n",
       "115      A146T           1  \n",
       "116      R164Q           1  \n",
       "117       G12V           1  \n",
       "118       G12V           1  \n",
       "119       G12V           1  \n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge the pub original sentences and test results (prediction of biobert model)\n",
    "\n",
    "final_re_output = pd.merge(original_sentences, predictions, on ='index', how='left')\n",
    "final_re_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
