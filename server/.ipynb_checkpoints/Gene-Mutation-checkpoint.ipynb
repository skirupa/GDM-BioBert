{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3605,
     "status": "ok",
     "timestamp": 1679075791770,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "RvaNWdrKgXjD",
    "outputId": "7fe66ab5-ecad-49ad-f8f1-6398703962bc"
   },
   "outputs": [],
   "source": [
    "#We extract the pubmed document in BioCJSON format\n",
    "import urllib3\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "pmcid = 'PMC2837563'\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "r = http.request('GET', f'https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocjson?pmcids={pmcid}')\n",
    "data = json.loads(r.data.decode('utf-8'))\n",
    "#data = json.dumps(data, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1679075793852,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "-3bUHWus1nUq",
    "outputId": "3f1c4784-c8ea-473b-d8bb-aefd779ea62b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEvery PMC id has passages.\\nEvery passage has many {infons, offset, text, sentences, annotations, relations}.\\nHere, text is the actual text we have to annotate. \\nEvery annotations has {id, infons, text, locations}. \\nHere infons has {identifier, type} (optional: ncbi-homologene if type is gene).\\nAlso locations has {offset, length}.\\n\\nPassages:\\n    a)infons - data realted article id, author, etc..\\n    b)offset - location index\\n    c)text - whole medical data (sentence) in which medical terms (gene name or disease name) are to be annotated.\\n    d)sentences - not required here\\n    e)annotations:\\n        1)id - key index\\n        2)infons:\\n              a)identifier\\n              b)type - \"Gene\" or \"Disease\" etc.,\\n        3)text - gene name or disease name etc., (Eg: \"K-Ras\")\\n        4)locations:\\n            a)offset - location index\\n            b)length - length of text. (Eg: len(\"tumours\") = 7) \\n    f)relations\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = json.dumps(data, indent=4)\n",
    "#print(output)\n",
    "\n",
    "'''\n",
    "Every PMC id has passages.\n",
    "Every passage has many {infons, offset, text, sentences, annotations, relations}.\n",
    "Here, text is the actual text we have to annotate. \n",
    "Every annotations has {id, infons, text, locations}. \n",
    "Here infons has {identifier, type} (optional: ncbi-homologene if type is gene).\n",
    "Also locations has {offset, length}.\n",
    "\n",
    "Passages:\n",
    "    a)infons - data realted article id, author, etc..\n",
    "    b)offset - location index\n",
    "    c)text - whole medical data (sentence) in which medical terms (gene name or disease name) are to be annotated.\n",
    "    d)sentences - not required here\n",
    "    e)annotations:\n",
    "        1)id - key index\n",
    "        2)infons:\n",
    "              a)identifier\n",
    "              b)type - \"Gene\" or \"Disease\" etc.,\n",
    "        3)text - gene name or disease name etc., (Eg: \"K-Ras\")\n",
    "        4)locations:\n",
    "            a)offset - location index\n",
    "            b)length - length of text. (Eg: len(\"tumours\") = 7) \n",
    "    f)relations\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3EkWKX8wA1b5"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(columns=['index','sentence'])\n",
    "df2 = pd.DataFrame(columns=['index','sentence','Gene','Mutation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSMLCcRaAlL_"
   },
   "source": [
    "**Annotations for Gene and Mutation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kbIqNKyCgkuF"
   },
   "outputs": [],
   "source": [
    "sentence_index = 0\n",
    "sentence_entities = {}\n",
    "for i in data['passages']:\n",
    "  if i['infons']['section_type'] != 'TABLE':\n",
    "    #filter the table segment\n",
    "    text = i['text']\n",
    "    # print(\"TEXT: \", text)\n",
    "    offset = i['offset']\n",
    "    # print(\"OFFSET: \", offset)    \n",
    "    annotations = i['annotations']\n",
    "    \n",
    "    annotations = sorted(annotations, key = lambda x: x['locations'][0]['offset'])\n",
    "    # print(\"ANNOTATIONS: \", annotations)\n",
    "    #Filter to only include gene-disease annotations\n",
    "    annotations = [annotation for annotation in annotations if ((annotation['infons']['type']=='Gene') or (annotation['infons']['type']=='Mutation'))]\n",
    "    #List all possible combinations of annotations\n",
    "    annots_combinations = list(combinations(annotations, 2))\n",
    "    # print(\"ANNOT COMBIMNATIONS: \", annots_combinations)\n",
    "    #Filter combinations to only include gene-disease combinations\n",
    "    annots_combinations = [annots for annots in annots_combinations if annots[0]['infons']['type'] != annots[1]['infons']['type']]\n",
    "    # print(\"ANNOT COMBINATIONS - GENE-DISEASE: \", annots_combinations)\n",
    "\n",
    "    #processing sentences\n",
    "    sentences = text.split('. ')\n",
    "    # print(sentences)\n",
    "    sentence_offset = {}\n",
    "    sentence_len = {}\n",
    "    prev_sent_offset = offset\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "      sentence_offset[sentence] = prev_sent_offset\n",
    "      current_sentence_offset = prev_sent_offset\n",
    "      sentence_len[sentence] = len(sentence)\n",
    "      current_sentence_len = len(sentence)\n",
    "      prev_sent_offset += len(sentence) + 2\n",
    "      \n",
    "\n",
    "      #Point to note: Duplicate sentences for as many combinations as present. \n",
    "      for annots in annots_combinations:\n",
    "        difference = 0\n",
    "        current_sentence = sentence\n",
    "        #sort the tuple\n",
    "        annots = sorted(annots, key = lambda x: x['locations'][0]['offset'])\n",
    "\n",
    "        entity_1 = annots[0]\n",
    "        entity_2 = annots[1]\n",
    "        \n",
    "        entity_1_offset = entity_1['locations'][0]['offset']\n",
    "        entity_2_offset = entity_2['locations'][0]['offset']\n",
    "\n",
    "        entity_1_dist = entity_1_offset - current_sentence_offset\n",
    "        entity_2_dist = entity_2_offset - current_sentence_offset\n",
    "\n",
    "        if (0 <= entity_1_dist <= ((current_sentence_len - len(entity_1['text'])) + 1)) and (0 <= entity_2_dist <= ((current_sentence_len - len(entity_2['text'])) + 1)):\n",
    "          #the pair of annotations fall within the sentence\n",
    "          sentence_entities[sentence_index] = (entity_1['text'], entity_2['text'])\n",
    "\n",
    "          entity_1_type = entity_1['infons']['type']\n",
    "          entity_1_length = entity_1['locations'][0]['length']\n",
    "          temp = '@'+ entity_1_type +'$'\n",
    "          entity_1_final_off = entity_1_dist \n",
    "          current_sentence = current_sentence[:entity_1_final_off] + \"@\" + entity_1_type + \"$\" + current_sentence[(entity_1_final_off + entity_1_length):]\n",
    "          difference += (entity_1_length - len(temp))\n",
    "          entity_2_type = entity_2['infons']['type']\n",
    "          entity_2_length = entity_2['locations'][0]['length']\n",
    "          temp = '@'+ entity_2_type +'$'\n",
    "          entity_2_final_off = entity_2_dist - (difference)\n",
    "          current_sentence = current_sentence[:entity_2_final_off] + \"@\" + entity_2_type + \"$\" + current_sentence[(entity_2_final_off + entity_2_length):]\n",
    "          difference += (entity_2_length - len(temp))\n",
    "          # tsv_writer_1.writerow([sentence_index, current_sentence])\n",
    "          row1 = [sentence_index,current_sentence]\n",
    "          df1.loc[len(df1)] = row1\n",
    "          if (entity_1['infons']['type'] == 'Gene'):\n",
    "            # print('Writing...', [sentence_index, sentence, entity_1['text'], entity_2['text']])\n",
    "            # tsv_writer_2.writerow([sentence_index, sentence, entity_1['text'], entity_2['text']])\n",
    "            row2 = [sentence_index, sentence, entity_1['text'], entity_2['text']]\n",
    "            df2.loc[len(df2)] = row2\n",
    "          else:\n",
    "            # print('Writing')\n",
    "            # tsv_writer_2.writerow([sentence_index, sentence, entity_2['text'], entity_1['text']])\n",
    "            row2 = [sentence_index, sentence, entity_2['text'], entity_1['text']]\n",
    "            df2.loc[len(df2)] = row2\n",
    "          sentence_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rax5aqQmrZp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/skirupa/Desktop/sem8/RSL-Lab/RSLLab-20230318T070434Z-001/RSLLab/project-sample\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('../biobert-pytorch/relation-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Oe0kay_JgprA"
   },
   "outputs": [],
   "source": [
    "df2.to_csv('pub_original_sentences_GM.tsv', sep=\"\\t\", index=False)\n",
    "df1.to_csv('inputGM/test.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZSYZQAAi5db"
   },
   "source": [
    "**BioBert Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/skirupa/Desktop/sem8/RSL-Lab/RSLLab-20230318T070434Z-001/RSLLab/biobert-pytorch/relation-extraction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all datasets including NER/RE/QA\n",
    "#!bash ./download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/skirupa/Desktop/sem8/RSL-Lab/RSLLab-20230318T070434Z-001/RSLLab/biobert-pytorch\n",
      "/home/skirupa/Desktop/sem8/RSL-Lab/RSLLab-20230318T070434Z-001/RSLLab/biobert-pytorch/relation-extraction\n",
      "*****  euadr  Preprocessing Start *****\n",
      "*****  euadr  Preprocessing Done *****\n",
      "*****  GAD  Preprocessing Start *****\n",
      "*****  GAD  Preprocessing Done *****\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('relation-extraction')\n",
    "print(os.getcwd())\n",
    "#To preprocess the datasets downloaded\n",
    "!bash ./preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8981,
     "status": "ok",
     "timestamp": 1679070353775,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "jPCXvCJLkLsy",
    "outputId": "b2ffa5ef-ecb7-4bd9-dbe3-640d4f36ddcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: pandas in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/skirupa/miniconda3/envs/env1/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install scikit-learn\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1679070381883,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "uv7yHEmWkPk8",
    "outputId": "678d9197-a2d7-41d0-fa94-9dc9a2532ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SAVE_DIR=./outputDM\n",
      "env: DATA=\"GAD\"\n",
      "env: SPLIT=\"1\"\n",
      "env: DATA_DIR=./inputDM\n",
      "env: ENTITY=${DATA}-${SPLIT}\n",
      "env: MAX_LENGTH=128\n",
      "env: BATCH_SIZE=32\n",
      "env: NUM_EPOCHS=3\n",
      "env: SAVE_STEPS=1000\n",
      "env: SEED=1\n"
     ]
    }
   ],
   "source": [
    "%env SAVE_DIR=./outputGM\n",
    "%env DATA=\"GAD\"\n",
    "%env SPLIT=\"1\"\n",
    "%env DATA_DIR=./inputGM\n",
    "%env ENTITY=${DATA}-${SPLIT}\n",
    "\n",
    "%env MAX_LENGTH=128\n",
    "%env BATCH_SIZE=32\n",
    "%env NUM_EPOCHS=3\n",
    "%env SAVE_STEPS=1000\n",
    "%env SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7095,
     "status": "ok",
     "timestamp": 1679070394106,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "Nap7OsDNomOM",
    "outputId": "4ba15580-e24e-4619-f13d-fb35ad41f6ed"
   },
   "outputs": [],
   "source": [
    "!python run_re.py --task_name SST-2 --config_name bert-base-cased --model_name_or_path dmis-lab/biobert-base-cased-v1.1 \\\n",
    "        --do_predict --data_dir ${DATA_DIR} \\\n",
    "        --output_dir ${SAVE_DIR} \\\n",
    "        --overwrite_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1679064615235,
     "user": {
      "displayName": "19PT24 - S KIRUPA",
      "userId": "12629547787028680648"
     },
     "user_tz": -330
    },
    "id": "QgHoYa6Noo9w",
    "outputId": "6af921bc-1802-4a23-bb95-187243510d51"
   },
   "outputs": [],
   "source": [
    "original_sentences = pd.read_csv('pub_original_sentences_GM.tsv', sep=\"\\t\")\n",
    "predictions = pd.read_csv('outputGM/test_results.txt', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX9DHYpyBbeC"
   },
   "outputs": [],
   "source": [
    "#Merge the pub original sentences and test results (prediction of biobert model)\n",
    "\n",
    "final_re_output = pd.merge(original_sentences, predictions, on ='index', how='left')\n",
    "final_re_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
